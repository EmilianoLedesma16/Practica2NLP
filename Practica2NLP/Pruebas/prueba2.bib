@article{liu2025stepwise,
  author = {Han Liu and Wei Zhang and Tian Liang and Xingyu Chen and Rui Wang and Zhiwei He and Zhuosheng Zhang and Haitao Mi and Dong Yu},
  title = {Stepwise Reflection: Improving LLM Logical Reasoning with Iterative Self-Feedback},
  journal = {Journal of Artificial Intelligence Research},
  year = {2025},
  volume = {68},
  pages = {123-145},
  doi = {10.48550/arXiv.2503.20001},
  abstract = {Enhancing the logical reasoning abilities of large language models (LLMs) is an ongoing challenge, especially for complex multi-step inference tasks. Traditional methods rely on scalar reward signals, which lack detailed qualitative insights into each reasoning step. In this paper, we introduce Stepwise Reflection, a novel approach that integrates iterative self-feedback mechanisms at each reasoning step. By leveraging natural language explanations to critique and refine responses dynamically, our approach improves decision-making without requiring external task-specific verifiers. Experimental evaluations on benchmarks such as AIME and GPQA show that Stepwise Reflection significantly enhances reasoning performance, outperforming standard scalar reward-based techniques.},
  keywords = {Artificial Intelligence, Large Language Models, Logical Reasoning},
  archivePrefix = {arXiv},
  eprint = {2503.20001},
  primaryClass = {cs.CL}
}
